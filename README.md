# Aim:

Development of a Python script compatible with multiple AI tools to benchmark and compare their outputs, and to generate actionable insights for informed tool selection.

# Algorithm / Approach:

Write and implement Python code that:

1.Integrates with multiple AI service APIs.

2.Sends standardized prompts or inputs.

3.Collects and stores the outputs from each provider.

4.Compares outputs on metrics like quality, tone, and accuracy.

5.Generates reports or logs with actionable insights.

# Objective:

Build a reusable Python module that:
a. Connects to multiple AI services via APIs.
b. Automates prompt submission to each service.
c. Gathers and stores responses in a structured format.
d. Implements comparison logic (e.g., text similarity, sentiment, readability).
e. Produces reports or visualizations for analysis.

# Detailed Example: Comparing Summarization Capabilities of OpenAI GPT-4 and Cohere

## Step 1: Define Use Case
We will compare how well OpenAI and Cohere summarize the same input text.

## Step 2: Setup API Access

1.Sign up at OpenAI and Cohere.

2.Generate API keys from their dashboards.

3.Create a .env file to store keys:
```
OPENAI_API_KEY=your_openai_key_here
COHERE_API_KEY=your_cohere_key_here
```
4.Install required libraries:
```
pip install openai cohere python-dotenv textblob scikit-learn
```
## Step 3: Create the Python Script
```
import os
from dotenv import load_dotenv
import openai
import cohere
from textblob import TextBlob
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json
```
### Load API keys from .env file
```
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
COHERE_API_KEY = os.getenv("COHERE_API_KEY")
```
### Initialize clients
```
openai.api_key = OPENAI_API_KEY
co = cohere.Client(COHERE_API_KEY)

def get_openai_summary(text):
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": f"Summarize this: {text}"}]
    )
    return response['choices'][0]['message']['content']

def get_cohere_summary(text):
    response = co.summarize(text=text)
    return response.summary

def analyze_text(summary):
    blob = TextBlob(summary)
    return {
        "word_count": len(summary.split()),
        "sentiment": blob.sentiment.polarity,
        "subjectivity": blob.sentiment.subjectivity
    }

def calculate_similarity(text1, text2):
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform([text1, text2])
    similarity_score = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])
    return similarity_score[0][0]
```
### Example text inputs
```
test_inputs = [
    "Artificial Intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, and self-correction.",
    "Climate change refers to long-term shifts in temperatures and weather patterns. These changes may be natural, but human activities have been the main driver of climate change since the 1800s."
]

results = []
for text in test_inputs:
    print(f"Processing input: {text[:50]}...")
    openai_output = get_openai_summary(text)
    cohere_output = get_cohere_summary(text)

    analysis_openai = analyze_text(openai_output)
    analysis_cohere = analyze_text(cohere_output)

    similarity = calculate_similarity(openai_output, cohere_output)

    result = {
        "input": text,
        "openai_summary": openai_output,
        "cohere_summary": cohere_output,
        "openai_analysis": analysis_openai,
        "cohere_analysis": analysis_cohere,
        "similarity_score": similarity
    }

    results.append(result)
```
### Save to file
```
with open("summary_comparison.json", "w") as f:
    json.dump(results, f, indent=4)

print("\nSummary comparison complete. Results saved to summary_comparison.json")
```
## Explanation:

We define reusable functions to interact with both APIs.

We use TextBlob for sentiment and subjectivity analysis.

TfidfVectorizer + cosine_similarity allows us to numerically measure how similar the two summaries are.

Final results are stored in a JSON file.

# Result:

Summaries generated by both AI models.

Comparative analysis stored including:

Word count

Sentiment polarity

Subjectivity

Cosine similarity score between summaries

# Conclusion:
This Python-based pipeline automates the evaluation of summarization quality across different AI tools. It demonstrates:

Seamless integration with multiple AI APIs.

Objective comparison using standard NLP metrics.

Actionable reporting to guide future tool usage decisions.

This template can be extended for tasks like translation, question answering, image captioning, etc., by modifying the use case and evaluation metrics accordingly.

